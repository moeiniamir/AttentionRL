{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            if self.classifier:\n",
    "                classifier_logits = self.classifier(self._get_obs(update_history=False)['history']['history'])\n",
    "                new_dist = torch.distributions.Categorical(logits=classifier_logits)\n",
    "                kl_bad = torch.distributions.kl.kl_divergence(self.whole_dist, new_dist)\n",
    "                kl_good = torch.distributions.kl.kl_divergence(self.prev_dist, new_dist)\n",
    "                reward = kl_good.item() - .3\n",
    "                self.prev_dist = new_dist\n",
    "                self.history.process_all()\n",
    "                if kl_bad < .5:\n",
    "                    end = True\n",
    "                else:\n",
    "                    end = False\n",
    "            else:\n",
    "                reward = self._reward_missed_soft()\n",
    "                reward = np.clip(reward, -10, 10)\n",
    "                end = True\n",
    "                \n",
    "            logs = self.get_logs(reward, True)\n",
    "            return self._get_obs(update_history=False), reward, end, False, {'logs':logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTMAEModel, ViTMAEConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/vit-mae-base')\n",
    "config = ViTMAEConfig.from_pretrained('facebook/vit-mae-base',\n",
    "                                             proxies={'http': '127.0.0.1:10809', 'https': '127.0.0.1:10809'},)\n",
    "model = ViTMAEModel.from_pretrained('facebook/vit-mae-base', config=config)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# loss = outputs.loss\n",
    "# mask = outputs.mask\n",
    "# ids_restore = outputs.ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Array, Pipe, connection\n",
    "parent_conn, child_conn = Pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.context import Process\n",
    "from multiprocessing import Array, Pipe, connection, SimpleQueue\n",
    "\n",
    "def classifier_tgt(pipe):\n",
    "    classifer = ViTClassifier().to(device2)\n",
    "    while True:\n",
    "        try:\n",
    "            x = pipe.recv()\n",
    "            out = classifer(x)\n",
    "            pipe.send(out)\n",
    "        except EOFError:\n",
    "            print('closed'*20)\n",
    "            pipe.close()\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parent_conn, child_conn = Pipe()\n",
    "    p = Process(target=classifier_tgt, args=(child_conn,), daemon=True)\n",
    "    p.start()\n",
    "    # child_conn.close()\n",
    "\n",
    "def surrogate_classifier(x):\n",
    "    parent_conn.send(x)\n",
    "    return parent_conn.recv()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    surrogate_classifier.device = device2\n",
    "    print(surrogate_classifier(torch.ones(3, 224, 224)).shape) #! neeeeeeeeeed RPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = COCODataset(train=True, indices=[\n",
    "                        4385], no_seg=False, fix_resize=(224-3*16, 224-3*16))\n",
    "env = TimeLimit(Environment(dataset,\n",
    "                            config['env_patch_size'],\n",
    "                            max_len=config['env_step_mem'],\n",
    "                            n_last_positions=config['n_last_positions']\n",
    "                            ),\n",
    "                config['env_step_limit_test'])\n",
    "obs, _ = env.reset()\n",
    "display.clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, *_ = env.step(Actions.LEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ts.data.Batch(\n",
    "                obs=[obs],\n",
    "                info=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy.actor.last.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(model, before, after):\n",
    "    print(before[0], len(before))\n",
    "hook.remove()\n",
    "hook = policy.actor.last.model[4].register_forward_hook(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_out = policy(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.9156,  11.7836,  -2.5068, -14.9789]], device='cuda:1',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meat loaf, meatloaf 0.5452646613121033\n"
     ]
    }
   ],
   "source": [
    "logitsg = classifier(env.current_image.unsqueeze(0).to(device2))\n",
    "probsg = torch.softmax(logitsg, -1)\n",
    "print(classifier.vit.config.id2label[logitsg.argmax().item()], probsg.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = classifier(out[0]['history']['history'].to(torch.float32).unsqueeze(0).to(device2))\n",
    "print(classifier.vit.config.id2label[logits.argmax().item()], torch.softmax(logits, -1).max().item(), probsg[0, logits.argmax().item()].item())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
