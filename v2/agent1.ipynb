{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import SupportsFloat, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.core import RenderFrame, ActType, ObsType\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from torchvision.models import resnet101\n",
    "from utils import image_net_postprocessing\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class COCODataset(D.Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        root = Path(root)\n",
    "\n",
    "        with open(root/'annotations/captions_train2017.json', 'r') as f:\n",
    "            images_info = json.load(f)\n",
    "        self.file_name_to_id = dict()\n",
    "        for image_info in images_info['images']:\n",
    "            self.file_name_to_id[image_info['file_name']] = image_info['id']\n",
    "\n",
    "        with open(root/'cap_dict.json', 'r') as f:\n",
    "            self.captions_dict = json.load(f)\n",
    "\n",
    "        if train:\n",
    "            self.image_files = glob.glob(os.path.join(root/'train2017', \"*.jpg\"))\n",
    "        else:\n",
    "            self.image_files = glob.glob(os.path.join(root/'val2017', \"*.jpg\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.image_files[index]\n",
    "        image = PIL.Image.open(image_file)\n",
    "        image = image.convert('RGB')\n",
    "        file_name = image_file.split('/')[-1]\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "        return image_tensor, self.file_name_to_id[file_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GradCam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Resnet101GradCam(torch.nn.Module):\n",
    "    def __init__(self, patch_size = (64, 64)):\n",
    "        super(Resnet101GradCam, self).__init__()\n",
    "        feature_extractor = resnet101(pretrained=True).to(device).eval()\n",
    "        self.vis = GradCAM(feature_extractor, device)\n",
    "        self.patch_size = patch_size\n",
    "        self.conv_mean = nn.Conv2d(1, 1, kernel_size=self.patch_size, stride=self.patch_size, padding=0,  bias=False)\n",
    "        self.conv_mean.weight = torch.nn.Parameter(torch.ones_like(self.conv_mean.weight)  / (patch_size[0] * patch_size[1]))\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        self.vis(x, None, postprocessing=image_net_postprocessing)[0]\n",
    "        self.vis.cam -= torch.min(self.vis.cam)\n",
    "        self.vis.cam /= torch.max(self.vis.cam)\n",
    "        resized_cam = transforms.Resize(size=(h, w))(self.vis.cam.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        number_of_rows = (h - 1) // self.patch_size[0] + 1\n",
    "        number_of_cols = (w - 1) // self.patch_size[1] + 1\n",
    "\n",
    "        scores = self.conv_mean(resized_cam)\n",
    "\n",
    "        softmax = nn.Softmax()\n",
    "\n",
    "        choice = torch.multinomial(scores.flatten(), 1).item()\n",
    "\n",
    "        row_choice, col_choice = choice // number_of_cols, choice % number_of_cols\n",
    "\n",
    "        return torch.tensor([[row_choice, col_choice]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "    STAY = 0\n",
    "    UP = 1\n",
    "    RIGHT = 2\n",
    "    DOWN = 3\n",
    "    LEFT = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, dataloader, device, patch_size=(64, 64), input_size=224):\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = iter(dataloader)\n",
    "        self.grad = Resnet101GradCam(patch_size).to(device).eval()\n",
    "        self.transform = transforms.Resize(input_size)\n",
    "        self.patch_size = patch_size\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            # Samples the batch\n",
    "            self.current_image, self.image_id = next(self.iterator)\n",
    "        except StopIteration:\n",
    "            # restart the iterator if the previous iterator is exhausted.\n",
    "            self.iterator = iter(self.dataloader)\n",
    "            self.current_image, self.image_id = next(self.iterator)\n",
    "\n",
    "        self.current_image = self.current_image.to(self.device)\n",
    "        self.image_id = str(self.image_id.item())\n",
    "        _, _, self.height, self.width = self.current_image.shape\n",
    "        self.captions = captions_dict[self.image_id]\n",
    "        self.row, self.col = [int(x) for x in self.grad(self.transform(self.current_image), self.height, self.width)[0]]\n",
    "        self.calculate_patch_histograms()\n",
    "\n",
    "\n",
    "    def calculate_patch_histograms(self):\n",
    "        max_row, max_col = self.height // self.patch_size[0], self.width // self.patch_size[1]\n",
    "\n",
    "        self.histograms = []\n",
    "\n",
    "        for i in range(max_row):\n",
    "            self.histograms.append([])\n",
    "            for j in range(max_col):\n",
    "                start_row, end_row = i * self.patch_size[0], (i + 1) * self.patch_size[0]\n",
    "                start_col, end_col = j * self.patch_size[1], (j + 1) * self.patch_size[1]\n",
    "\n",
    "                selected_patch = self.current_image[:, :, start_row: end_row, start_col: end_col].detach().clone()\n",
    "\n",
    "                hue = self.rgb2h(selected_patch).cpu()\n",
    "                self.histograms[-1].append(np.histogram(hue, bins=10, range=(0., 1.), density=True)[0])\n",
    "\n",
    "\n",
    "    def measure_similarity(self):\n",
    "        current_patch_histogram = self.histograms[self.row][self.col]\n",
    "        max_row, max_col = self.height // self.patch_size[0], self.width // self.patch_size[1]\n",
    "        similarity_matrix = []\n",
    "\n",
    "        for i in range(max_row):\n",
    "            similarity_matrix.append([])\n",
    "            for j in range(max_col):\n",
    "                p_value = ks_2samp(current_patch_histogram,  self.histograms[i][j]).pvalue\n",
    "                similarity_matrix[-1].append(p_value)\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def get_patch(self):\n",
    "        start_row, end_row = self.row * self.patch_size[0], (self.row + 1) * self.patch_size[0]\n",
    "        start_col, end_col = self.col * self.patch_size[1], (self.col + 1) * self.patch_size[1]\n",
    "        return self.current_image[:, :, start_row: end_row, start_col: end_col]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == Actions.UP:\n",
    "            self.row = self.row - 1\n",
    "        elif action == Actions.RIGHT:\n",
    "            self.col = self.col + 1\n",
    "        elif action == Actions.DOWN:\n",
    "            self.row = self.row + 1\n",
    "        elif action == Actions.LEFT:\n",
    "            self.col = self.col - 1\n",
    "        else:\n",
    "            pass #TODO STAY\n",
    "\n",
    "        return self.get_patch()\n",
    "\n",
    "    def visualize(self, alpha=0.4):\n",
    "        start_row, end_row = self.row * self.patch_size[0], (self.row + 1) * self.patch_size[0]\n",
    "        start_col, end_col = self.col * self.patch_size[1], (self.col + 1) * self.patch_size[1]\n",
    "\n",
    "        mask = torch.zeros_like(self.current_image)\n",
    "        mask[:, :, start_row: end_row, start_col: end_col] = 1\n",
    "\n",
    "        return alpha * mask + (1 - alpha) * self.current_image\n",
    "\n",
    "    def rgb2h(self, rgb: torch.Tensor) -> torch.Tensor:\n",
    "        cmax, cmax_idx = torch.max(rgb, dim=1, keepdim=True)\n",
    "        cmin = torch.min(rgb, dim=1, keepdim=True)[0]\n",
    "        delta = cmax - cmin\n",
    "        hsv_h = torch.empty_like(rgb[:, 0:1, :, :])\n",
    "        cmax_idx[delta == 0] = 3\n",
    "        hsv_h[cmax_idx == 0] = (((rgb[:, 1:2] - rgb[:, 2:3]) / delta) % 6)[cmax_idx == 0]\n",
    "        hsv_h[cmax_idx == 1] = (((rgb[:, 2:3] - rgb[:, 0:1]) / delta) + 2)[cmax_idx == 1]\n",
    "        hsv_h[cmax_idx == 2] = (((rgb[:, 0:1] - rgb[:, 1:2]) / delta) + 4)[cmax_idx == 2]\n",
    "        hsv_h[cmax_idx == 3] = 0.\n",
    "        hsv_h /= 6.\n",
    "        hsv_s = torch.where(cmax == 0, torch.tensor(0.).type_as(rgb), delta / cmax)\n",
    "        hsv_v = cmax\n",
    "        return hsv_h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "defining the environment as an implmentation of the gym.Env class\n",
    "'''\n",
    "class TraversalEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human'], \"render_fps\": 2}\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        pass\n",
    "\n",
    "    def render(self) -> RenderFrame | list[RenderFrame] | None:\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
