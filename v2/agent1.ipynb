{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.922521172Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.113529464Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.931609958Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.124617050Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script pack_existing_segs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.300770945Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.921840758Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.335443232Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.932201025Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "import einops\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tianshou.policy import PPOPolicy\n",
    "import uuid\n",
    "from tianshou.utils import WandbLogger, LazyLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from customs import CustomDQNPolicy, CustomOffpolicyTrainer, CustomSubprocVectorEnv\n",
    "from dataset import *\n",
    "from env.env import *\n",
    "from networks.qnet import *\n",
    "from networks.vit import ViTTrailEncoder\n",
    "from networks.SimpleAC import Actor as SimpleActor, Critic as SimpleCritic\n",
    "from tianshou.utils.net.common import ActorCritic\n",
    "from tianshou.utils.net.discrete import Actor, Critic, IntrinsicCuriosityModule\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "import torch\n",
    "import tianshou as ts\n",
    "from networks.AC import MulActor, CAAdjCritic, CrossAttentionActor, CrossAttentionCritic\n",
    "from networks.path_vit import BaseNetwork as MulBaseNetwork\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from networks.simmim import BaseNetwork as SimmimBase\n",
    "from networks.beit import BaseNetwork as BeitBase\n",
    "from networks.mae import BaseNetwork as MaeBase, OrderEmbeddingBaseNetwork as OrdMaeBase\n",
    "import utils\n",
    "from utils import get_wandb\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "# reporter = MemReporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.301058023Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.300294091Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.335729135Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.331166072Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    wandb_logger = None\n",
    "if '__file__' in globals():\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "if '__file__' in globals() and __name__ == '__main__':\n",
    "    torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainFunction:\n",
    "    def init(self, policy=None, eps_train=None, train_phases=None, trainer=None):\n",
    "        self.policy = policy\n",
    "        self.eps_train = eps_train\n",
    "        self.train_phases = train_phases\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def __call__(self, epoch, env_step):\n",
    "        if self.eps_train:\n",
    "            self.policy.set_eps(self.eps_train[epoch - 1])\n",
    "\n",
    "        # if epoch >= self.train_phases[1]:\n",
    "        #     policy = self.policy\n",
    "        #     for param in policy.actor.preprocess.vit.encoder.layer[-6:].parameters():\n",
    "        #         param.requires_grad = True\n",
    "        #     for param in policy.actor.preprocess.vit.layernorm.parameters():\n",
    "        #         param.requires_grad = True\n",
    "        # if epoch >= self.train_phases[0]:\n",
    "        #     policy = self.policy\n",
    "        #     policy.actor.preprocess.vit.embeddings.order_embeddings.requires_grad = True\n",
    "        if wandb_logger.use_wandb and epoch >= 0 and wandb_logger.wandb_run.step % 500 == 0:\n",
    "            torch.save(self.policy.state_dict(),\n",
    "                       f'saved/policy_{wandb_logger.wandb_run.name}_{wandb_logger.wandb_run.step}.pt')\n",
    "\n",
    "        wandb_logger.write(\n",
    "            'train', self.trainer.env_step, {\"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.378901552Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.349735874Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.378122737Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.375907458Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "def get_DQN_policy(config):\n",
    "    config.update({\n",
    "        \"n_step\": 1,\n",
    "        \"target_freq\": 1200,\n",
    "    })\n",
    "\n",
    "    action_count = len(Actions)\n",
    "    net = Q_network(action_count).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=config['lr'])\n",
    "    policy = CustomDQNPolicy(net, optim, discount_factor=config['gamma'], estimation_step=config['n_step'],\n",
    "                             target_update_freq=config['target_freq'], polyak=None)\n",
    "    # register_hooks(policy.model)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def get_PPO_policy(config):\n",
    "    config.update({\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"gae_batch_size\": config['batch_size'],\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"vf_coef\": 0.25,\n",
    "        \"ent_coef\": .0001,\n",
    "        \"rew_norm\": False,\n",
    "        \"eps_clip\": 0.1,\n",
    "        \"value_clip\": False,\n",
    "        \"dual_clip\": 3,\n",
    "        \"norm_adv\": False,\n",
    "        \"recompute_adv\": False,\n",
    "    })\n",
    "\n",
    "    # old base, built-in actor and critic\n",
    "    # base_net = ViTTrailEncoder().to(device)\n",
    "    # actor = Actor(base_net, len(Actions), device=device, softmax_output=False, hidden_sizes=[])\n",
    "    # critic = Critic(base_net, device=device, hidden_sizes=[])\n",
    "\n",
    "    # old base, SimpleAC\n",
    "    # base_net = ViTTrailEncoder().to(device)\n",
    "    # actor = SimpleActor(base_net, len(Actions), device=device, softmax_output=False, hidden_sizes=[256])\n",
    "    # critic = SimpleCritic(base_net, device=device, hidden_sizes=[512])\n",
    "\n",
    "    # new base, built-in actor and critic\n",
    "    # base_net = MulBaseNetwork(config['env_patch_size']).to(device)\n",
    "    # actor = Actor(base_net, len(Actions), device=device, softmax_output=False)\n",
    "    # critic = Critic(base_net, device=device)\n",
    "\n",
    "    # new base, SimpleAC\n",
    "    # base_net = MulBaseNetwork(config['env_patch_size']).to(device)\n",
    "    # actor = SimpleActor(base_net, len(Actions), device=device, softmax_output=False, hidden_sizes=[256, 256])\n",
    "    # critic = SimpleCritic(base_net, device=device, hidden_sizes=[512, 512])\n",
    "\n",
    "    # new base, mulAC\n",
    "    # base_net = MulBaseNetwork(config['env_patch_size']).to(device)\n",
    "    # actor = MulActor(base_net.vit.config.hidden_size, base_net).to(device)\n",
    "    # critic = MulCritic(base_net.vit.config.hidden_size, base_net).to(device)\n",
    "\n",
    "    # simmim base, mulAC\n",
    "    # base_net = SimmimBase(config['env_patch_size']).to(device)\n",
    "    # actor = MulActor(base_net.vit.config.hidden_size, base_net).to(device)\n",
    "    # critic = MulCritic(base_net.vit.config.hidden_size, base_net).to(device)\n",
    "\n",
    "    # beit base, SimpleAC\n",
    "    # base_net = BeitBase(config['env_patch_size'], False).to(device)\n",
    "    # actor = SimpleActor(base_net, len(Actions), device=device, softmax_output=False, hidden_sizes=[256, 256])\n",
    "    # critic = SimpleCritic(base_net, device=device, hidden_sizes=[512, 512])\n",
    "\n",
    "    # beit base, mulAC\n",
    "    # base_net = BeitBase(config['env_patch_size'], False).to(device)\n",
    "    # actor = MulActor(base_net.vit.config.vocab_size, base_net).to(device)\n",
    "    # critic = MulCritic(base_net.vit.config.vocab_size, base_net).to(device)\n",
    "\n",
    "    # mae base, SimpleAC\n",
    "    # base_net = MaeBase(config['env_patch_size'],\n",
    "    #                    config['n_last_positions']).to(device)\n",
    "    # actor = SimpleActor(base_net, len(Actions), device=device,\n",
    "    #                     softmax_output=False, hidden_sizes=[1024, 1024])\n",
    "    # critic = SimpleCritic(base_net, device=device, hidden_sizes=[2048, 2048])\n",
    "    # actor = SimpleActor(base_net, len(Actions), device=device,\n",
    "    #                     softmax_output=False, hidden_sizes=[256, 256])\n",
    "    # critic = SimpleCritic(base_net, device=device, hidden_sizes=[512, 512])\n",
    "    # head_ord_emb_params = [base_net.vit.embeddings.order_embeddings] + \\\n",
    "    #     list(actor.last.parameters()) + list(critic.last.parameters())\n",
    "    # pretrained_params = list(\n",
    "    #     base_net.vit.encoder.layer[-6:].parameters()) + list(base_net.vit.layernorm.parameters())\n",
    "    # optim1 = torch.optim.Adam(\n",
    "    #     head_ord_emb_params, lr=config['lr'], eps=1e-5\n",
    "    # )\n",
    "    # optim2 = torch.optim.Adam(\n",
    "    #     pretrained_params, lr=config['ft_lr'], eps=1e-5\n",
    "    # )\n",
    "\n",
    "    # mae ord emb, TransformerAC\n",
    "    base_net = OrdMaeBase(config['env_patch_size'],\n",
    "                          config['n_last_positions']).to(device)\n",
    "    actor = CrossAttentionActor(base_net, base_net.output_dim).to(device)\n",
    "    critic = CrossAttentionCritic(base_net, base_net.output_dim).to(device)\n",
    "\n",
    "    # class OptimizerWrapper:\n",
    "    #     def __init__(self, *optims):\n",
    "    #         self.optims = optims\n",
    "\n",
    "    #     def zero_grad(self):\n",
    "    #         for optim in self.optims:\n",
    "    #             optim.zero_grad()\n",
    "\n",
    "    #     def step(self):\n",
    "    #         for optim in self.optims:\n",
    "    #             optim.step()\n",
    "    # optim = OptimizerWrapper(optim1, optim2)\n",
    "\n",
    "    optim = torch.optim.Adam(\n",
    "    ActorCritic(actor, critic).parameters(), lr=config['lr'], eps=1e-5, \n",
    "    betas=[.9, .9]\n",
    "    )\n",
    "\n",
    "    def dist(p):\n",
    "        return torch.distributions.Categorical(logits=p)\n",
    "\n",
    "    # policy = PPOPolicy(\n",
    "    policy = TempPPO(\n",
    "        actor=actor,\n",
    "        critic=critic,\n",
    "        optim=optim,\n",
    "        dist_fn=dist,\n",
    "        discount_factor=config['gamma'],\n",
    "        gae_lambda=config['gae_lambda'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        vf_coef=config['vf_coef'],\n",
    "        ent_coef=config['ent_coef'],\n",
    "        reward_normalization=config['rew_norm'],\n",
    "        action_scaling=False,\n",
    "        eps_clip=config['eps_clip'],\n",
    "        value_clip=config['value_clip'],\n",
    "        dual_clip=config['dual_clip'],\n",
    "        advantage_normalization=config['norm_adv'],\n",
    "        recompute_advantage=config['recompute_adv'],\n",
    "        max_batchsize=config['gae_batch_size'],\n",
    "    ).to(device)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "from tianshou.data import Batch\n",
    "from tianshou.data import Batch, ReplayBuffer, to_torch_as\n",
    "\n",
    "sample_batch = None\n",
    "\n",
    "\n",
    "class TempPPO(PPOPolicy):\n",
    "    def __init__(\n",
    "        self, *args, **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_rew_mean = -np.inf\n",
    "        self.prev_max_filename = None\n",
    "    \n",
    "    def process_fn(\n",
    "        self, batch: Batch, buffer: ReplayBuffer, indices: np.ndarray\n",
    "    ) -> Batch:\n",
    "        global sample_batch\n",
    "        sample_batch = batch.copy()\n",
    "        if self._recompute_adv:\n",
    "            # buffer input `buffer` and `indices` to be used in `learn()`.\n",
    "            self._buffer, self._indices = buffer, indices\n",
    "        batch = self._compute_returns(batch, buffer, indices)\n",
    "        batch.act = to_torch_as(batch.act, batch.v_s)\n",
    "        with torch.no_grad():\n",
    "            batch.logp_old = self(batch).dist.log_prob(batch.act)\n",
    "        return batch\n",
    "\n",
    "    def learn(  # type: ignore\n",
    "            self, batch: Batch, batch_size: int, repeat: int, **kwargs: Any\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        losses, clip_losses, vf_losses, ent_losses = [], [], [], []\n",
    "        for step in range(repeat):\n",
    "            if self._recompute_adv and step > 0:\n",
    "                batch = self._compute_returns(\n",
    "                    batch, self._buffer, self._indices)\n",
    "            for i, minibatch in enumerate(batch.split(batch_size, merge_last=True)):\n",
    "                # calculate loss for actor\n",
    "                self.actor.preprocess.store_output = True\n",
    "                dist = self(minibatch).dist\n",
    "                \n",
    "                # print(torch.cuda.memory_summary())\n",
    "\n",
    "                if self._norm_adv:\n",
    "                    mean, std = minibatch.adv.mean(), minibatch.adv.std()\n",
    "                    minibatch.adv = (minibatch.adv -\n",
    "                                     mean) / (std + self._eps)  # per-batch norm\n",
    "                ratio = (dist.log_prob(minibatch.act) -\n",
    "                         minibatch.logp_old).exp().float()\n",
    "                ratio = ratio.reshape(ratio.size(0), -1).transpose(0, 1)\n",
    "                surr1 = ratio * minibatch.adv\n",
    "                surr2 = ratio.clamp(\n",
    "                    1.0 - self._eps_clip, 1.0 + self._eps_clip\n",
    "                ) * minibatch.adv\n",
    "                if self._dual_clip:\n",
    "                    clip1 = torch.min(surr1, surr2)\n",
    "                    clip2 = torch.max(clip1, self._dual_clip * minibatch.adv)\n",
    "                    clip_loss = - \\\n",
    "                        torch.where(minibatch.adv < 0, clip2, clip1).mean()\n",
    "                else:\n",
    "                    clip_loss = -torch.min(surr1, surr2).mean()\n",
    "                # calculate loss for critic\n",
    "                \n",
    "                value = self.critic(minibatch.obs).flatten()\n",
    "\n",
    "                if i == 0 and wandb_logger.use_wandb:\n",
    "                    rew_min, rew_max, rew_mean, rew_std = minibatch.rew.min(\n",
    "                    ), minibatch.rew.max(), minibatch.rew.mean(), minibatch.rew.std()\n",
    "                    adv_min, adv_max, adv_mean, adv_std = minibatch.adv.min().cpu().item(), minibatch.adv.max().cpu().item(), minibatch.adv.mean().cpu().item(), minibatch.adv.std().cpu().item()\n",
    "                    ret_min, ret_max, ret_mean, ret_std = minibatch.returns.min().cpu().item(), minibatch.returns.max().cpu().item(), minibatch.returns.mean().cpu().item(), minibatch.returns.std().cpu().item()\n",
    "                    pred_ret_min, pred_ret_max, pred_ret_mean, pred_ret_std = value.min().cpu().item(), value.max().cpu().item(), value.mean().cpu().item(), value.std().cpu().item()\n",
    "                    pred_ret_of_min, pred_ret_of_max = value[minibatch.returns.argmin()].cpu().item(), value[minibatch.returns.argmax()].cpu().item()\n",
    "                    # residual_var = ((minibatch.returns - value).var() / minibatch.returns.var()).cpu().item()\n",
    "                    value_corr = torch.corrcoef(torch.vstack([minibatch.returns, value]))[0, 1].cpu().item()\n",
    "                    rel_entropy = dist.entropy().mean().cpu().item()/(torch.distributions.Categorical(logits=torch.tensor([1, 1, 1, 1])).entropy())\n",
    "                    \n",
    "                    effective_pos_adv_ratio = (((ratio<1.0 + self._eps_clip) & (minibatch.adv>0)).sum()/(minibatch.adv>0).sum()).cpu().item()\n",
    "                    effective_neg_adv_ratio = (((ratio>1.0 - self._eps_clip) & (ratio < self._dual_clip) & (minibatch.adv<0)).sum()/(minibatch.adv<0).sum()).cpu().item()\n",
    "                    pos_adv_mean = minibatch.adv[minibatch.adv>0].mean().cpu().item()\n",
    "                    neg_adv_mean = minibatch.adv[minibatch.adv<0].mean().cpu().item()\n",
    "                    \n",
    "                    wandb_logger.wandb_run.log({\n",
    "                        'reward/min': rew_min,\n",
    "                        'reward/max': rew_max,\n",
    "                        'reward/mean': rew_mean,\n",
    "                        'reward/std': rew_std,\n",
    "                        'advantage/min': adv_min,\n",
    "                        'advantage/max': adv_max,\n",
    "                        'advantage/mean': adv_mean,\n",
    "                        'advantage/std': adv_std,\n",
    "                        'return/min': ret_min,\n",
    "                        'return/max': ret_max,\n",
    "                        'return/mean': ret_mean,\n",
    "                        'return/std': ret_std,\n",
    "                        'predicted_return/min': pred_ret_of_min,\n",
    "                        'predicted_return/max': pred_ret_of_max,\n",
    "                        'predicted_return/mean': pred_ret_mean,\n",
    "                        'predicted_return/std': pred_ret_std,\n",
    "                        'residual_var': value_corr,\n",
    "                        'rel_entropy': rel_entropy,\n",
    "                        'effective_pos_adv_ratio': effective_pos_adv_ratio,\n",
    "                        'effective_neg_adv_ratio': effective_neg_adv_ratio,\n",
    "                        'pos_adv_mean': pos_adv_mean,\n",
    "                        'neg_adv_mean': neg_adv_mean,\n",
    "                    })\n",
    "                if wandb_logger.use_wandb and minibatch.rew.mean() > self.max_rew_mean:\n",
    "                    self.max_rew_mean = rew_mean\n",
    "                    filename = f'saved/policy_max_{wandb_logger.wandb_run.name}_{wandb_logger.wandb_run.step}.pt'\n",
    "                    torch.save(self.state_dict(), filename)\n",
    "                    if self.prev_max_filename:\n",
    "                        try:\n",
    "                            os.remove(self.prev_max_filename)\n",
    "                        except:\n",
    "                            pass\n",
    "                    self.prev_max_filename = filename\n",
    "\n",
    "                if self._value_clip:\n",
    "                    v_clip = minibatch.v_s + \\\n",
    "                        (value - minibatch.v_s).clamp(-self._eps_clip, self._eps_clip)\n",
    "                    vf1 = (minibatch.returns - value).pow(2)\n",
    "                    vf2 = (minibatch.returns - v_clip).pow(2)\n",
    "                    vf_loss = torch.max(vf1, vf2).mean()\n",
    "                else:\n",
    "                    vf_loss = (minibatch.returns - value).pow(2).mean()\n",
    "                # calculate regularization and overall loss\n",
    "                ent_loss = dist.entropy().mean()\n",
    "                loss = clip_loss + self._weight_vf * vf_loss \\\n",
    "                    - self._weight_ent * ent_loss\n",
    "                self.optim.zero_grad()\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                if self._grad_norm:  # clip large gradient\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self._actor_critic.parameters(), max_norm=self._grad_norm\n",
    "                    )\n",
    "                self.optim.step()\n",
    "                \n",
    "                clip_losses.append(clip_loss.item())\n",
    "                vf_losses.append(vf_loss.item())\n",
    "                ent_losses.append(ent_loss.item())\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return {\n",
    "            \"loss\": losses,\n",
    "            \"loss/clip\": clip_losses,\n",
    "            \"loss/vf\": vf_losses,\n",
    "            \"loss/ent\": ent_losses,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_offpolicy_trainer(policy, config, wandb_logger, train_fn):\n",
    "    replay_buffer = ts.data.VectorReplayBuffer(\n",
    "        config['buffer_size'], len(config['train_envs']))\n",
    "    train_collector = ts.data.Collector(\n",
    "        policy, config['train_envs'], replay_buffer, exploration_noise=True)\n",
    "\n",
    "    config.update({\n",
    "        \"eps_start\": 0.9,\n",
    "        \"eps_end\": 0.05,\n",
    "        'update_per_step': 1 / config['step_per_collect'] * 4,\n",
    "        \"eps_train\": np.linspace(0.9, 0.05, 2000),\n",
    "        \"eps_test\": 0,\n",
    "    })\n",
    "    trainer = CustomOffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=None,\n",
    "        episode_per_test=1,  # dummy\n",
    "        max_epoch=config['epoch'],\n",
    "        step_per_epoch=config['step_per_epoch'],\n",
    "        step_per_collect=config['step_per_collect'],\n",
    "        batch_size=config['batch_size'],\n",
    "        update_per_step=config['update_per_step'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=lambda epoch, env_step: policy.set_eps(config['eps_test']),\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def get_onpolicy_trainer(policy, config, wandb_logger, train_fn):\n",
    "    replay_buffer = ts.data.VectorReplayBuffer(\n",
    "        config['buffer_size'],\n",
    "        len(config['train_envs']),\n",
    "    )\n",
    "    train_collector = ts.data.Collector(\n",
    "        policy, config['train_envs'], replay_buffer, exploration_noise=True)\n",
    "    test_collector = ts.data.Collector(policy, config['test_envs'])\n",
    "\n",
    "    config['train_envs'].mix_envs(list(range(len(Actions))), config['env_step_limit'])\n",
    "\n",
    "    config.update({\n",
    "        'repeat_per_collect': 5,\n",
    "    })\n",
    "\n",
    "    trainer = OnpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=config['epoch'],\n",
    "        step_per_epoch=config['step_per_epoch'],\n",
    "        repeat_per_collect=config['repeat_per_collect'],\n",
    "        batch_size=config['batch_size'],\n",
    "        step_per_collect=config['step_per_collect'],\n",
    "        episode_per_test=config['episode_per_test'],\n",
    "        train_fn=train_fn,\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  # Policy Initialization########\n",
    "    _bs = 700\n",
    "    _envs = 50\n",
    "    _steps = _bs * 1\n",
    "    config = {\n",
    "        \"lr\": 2e-5,\n",
    "        \"ft_lr\": 2e-5,\n",
    "        \"epoch\": 2000,\n",
    "        \"batch_size\": _bs,\n",
    "        \"train_phases\": [],\n",
    "        \"gamma\": 0.98,\n",
    "        \"buffer_size\": _steps,\n",
    "        \"env_step_limit\": 500,\n",
    "        \"env_step_limit_test\": 200,\n",
    "        \"env_step_mem\": 500,\n",
    "        \"env_patch_size\": (16, 16),\n",
    "        'num_train_envs': _envs,\n",
    "        'step_per_collect': _steps,\n",
    "        \"step_per_epoch\": 5 * _steps,\n",
    "        'n_last_positions': 20,\n",
    "        'episode_per_test': 5,\n",
    "    }\n",
    "\n",
    "    dataset_test = COCODataset(train=False, length=config['episode_per_test'], no_seg=False, fix_resize=(224-3*16, 224-3*16)) \n",
    "    # dataset = COCODataset(train=True, indices=[12], no_seg=False, fix_resize=(224-3*16, 224-3*16))\n",
    "    dataset = COCODataset(train=True, length=3000, no_seg=False, fix_resize=(224-3*16, 224-3*16))\n",
    "    \n",
    "\n",
    "    def env_creator():\n",
    "        torch.manual_seed(os.getpid())\n",
    "        return TimeLimit(\n",
    "            Environment(\n",
    "                dataset, config['env_patch_size'], max_len=config['env_step_mem'], n_last_positions=config['n_last_positions']),\n",
    "            config['env_step_limit'])\n",
    "\n",
    "    train_envs = CustomSubprocVectorEnv(\n",
    "        [env_creator for _ in range(config['num_train_envs'])])\n",
    "    config['train_envs'] = train_envs\n",
    "    config['test_envs'] = ts.env.DummyVectorEnv(\n",
    "        [lambda: TimeLimit(Environment(dataset_test, config['env_patch_size'], max_len=config['env_step_mem'], n_last_positions=config['n_last_positions']),\n",
    "                           config['env_step_limit_test']) for _ in range(1)])\n",
    "\n",
    "    # policy = get_DQN_policy()\n",
    "    policy = get_PPO_policy(config)\n",
    "    \n",
    "    if '__file__' not in globals(): #notebook\n",
    "        print(policy.load_state_dict(torch.load(\"./saved/policy_crisp-fog-210_2500.pt\", map_location=device)))\n",
    "        pass\n",
    "    else: # run\n",
    "        # print(policy.load_state_dict(torch.load(\"./saved/policy_wobbly-cherry-209_5000.pt\", map_location=device)))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = torch.compile(policy)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  # TTTTTTTTTTTTTTTTTTTTTTTTTTTraining########\n",
    "    # if '__file__' in globals():\n",
    "    use_wandb = False\n",
    "    wandb_logger = get_wandb(use_wandb)\n",
    "\n",
    "    # trainer = get_offpolicy_trainer(policy)\n",
    "    train_fn = TrainFunction()\n",
    "    trainer = get_onpolicy_trainer(policy, config, wandb_logger, train_fn)\n",
    "    train_fn.init(\n",
    "        policy, None, config['train_phases'], trainer)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb_logger.wandb_run.config.update(config)\n",
    "\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import IPython.display as display\n",
    "\n",
    "if __name__ == \"__main__\" and '__file__' not in globals():\n",
    "    display.clear_output(wait=True)\n",
    "    seg_out = widgets.Output()\n",
    "    img_out = widgets.Output()\n",
    "    stat_out = widgets.Output()\n",
    "    box = widgets.Box(children=[img_out, seg_out])\n",
    "    display.display(box)\n",
    "\n",
    "    policy.eval()\n",
    "\n",
    "    ''' \n",
    "    test time modifications\n",
    "    '''\n",
    "    fa = None\n",
    "    def test_dist_fn(logits):\n",
    "        global fa\n",
    "        probs = nn.functional.softmax(logits).squeeze().cpu().numpy()\n",
    "        if fa is None:\n",
    "            fa = plt.subplots(figsize=(2, 2))\n",
    "            plt.figure(figsize=(2,2))\n",
    "            fa[1].set_xlim([-1,1])\n",
    "            fa[1].set_ylim([-1,1])\n",
    "            fa[1].quiver([0,0,0,0], [0,0,0,0], [0, probs[1], 0, -probs[3]], [probs[0], 0, -probs[2], 0], scale=1)\n",
    "        else:\n",
    "            fa[1].get_children()[0].set_UVC([0, probs[1], 0, -probs[3]], [probs[0], 0, -probs[2], 0])\n",
    "        display.display(fa[0])\n",
    "        input()\n",
    "        # return torch.distributions.Categorical(probs=torch.distributions.transforms.SoftmaxTransform()(logits.pow(4)))\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "    # policy.dist_fn = test_dist_fn\n",
    "    # policy.set_eps(config['eps_test'])\n",
    "    ''''''\n",
    "\n",
    "    dataset = COCODataset(train=True, indices=[\n",
    "                          27], no_seg=False, fix_resize=(224-3*16, 224-3*16))\n",
    "    env = TimeLimit(Environment(dataset,\n",
    "                                config['env_patch_size'],\n",
    "                                max_len=config['env_step_mem'],\n",
    "                                n_last_positions=config['n_last_positions']\n",
    "                                ),\n",
    "                    config['env_step_limit_test'])\n",
    "    collector = ts.data.Collector(policy, env, exploration_noise=False)\n",
    "\n",
    "    with seg_out:\n",
    "        utils.show_masks_on_image(einops.rearrange(\n",
    "            env.unwrapped.current_image, 'c h w -> h w c'), env.unwrapped.current_seg.cpu().numpy())\n",
    "    with img_out:\n",
    "        collector.collect(n_episode=1, render=1 / 300)\n",
    "        # collector.collect(n_step=1, render=1 / 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
