{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.922521172Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.113529464Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.931609958Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.124617050Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script pack_existing_segs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.300770945Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.921840758Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.335443232Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.932201025Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "from tianshou.policy import PPOPolicy\n",
    "import uuid\n",
    "from tianshou.utils import WandbLogger, LazyLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from customs import custom_offpolicy_trainer, CustomDQNPolicy, CustomOffpolicyTrainer\n",
    "from dataset import *\n",
    "from env import *\n",
    "from networks.q_network import *\n",
    "from tianshou.utils.net.common import ActorCritic\n",
    "from tianshou.utils.net.discrete import Actor, Critic, IntrinsicCuriosityModule\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "import torch\n",
    "import tianshou as ts\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.301058023Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.300294091Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.335729135Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.331166072Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# if username is server use cuda\n",
    "if os.environ['USER'] == 'server':\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "# if __name__ == '__main__':\n",
    "#     torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wandb(use_wandb):\n",
    "    if use_wandb:\n",
    "        wandb_logger = WandbLogger(\n",
    "            train_interval=1,\n",
    "            test_interval=1,\n",
    "            update_interval=1,\n",
    "            project=\"AttentionRL\",\n",
    "        )\n",
    "        wandb_logger.load(SummaryWriter(\"./logs\"))\n",
    "    else:\n",
    "        wandb_logger = LazyLogger()\n",
    "\n",
    "    return wandb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_fn(policy, eps_train, inital_phase):\n",
    "    def train_fn(epoch, env_step):\n",
    "        if eps_train:\n",
    "            policy.set_eps(eps_train[epoch - 1])\n",
    "\n",
    "        if epoch >= inital_phase:\n",
    "            for param in policy.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        if epoch >= 0 and epoch % 5 == 0:\n",
    "            torch.save(policy.state_dict(), f'saved/policy_{wandb_logger.wandb_run.name if use_wandb else uuid.uuid4().hex}.pt')\n",
    "\n",
    "        wandb_logger.write('train', trainer.env_step, {\"epoch\": epoch})\n",
    "    return train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.378901552Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.349735874Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.378122737Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.375907458Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "def register_hooks(model):\n",
    "    hook0 = model.dueling_head.Q.model[-1].register_forward_hook(lambda self, input, output: print(f\"Q: {output}\"))\n",
    "    hook1 = model.dueling_head.V.model[-1].register_forward_hook(lambda self, input, output: print(f\"V: {output}\"))\n",
    "    # hook2 = model.dueling_head.model.model[0].register_forward_hook(lambda self, input, output: print(f\"net: {output}\"))\n",
    "\n",
    "def get_DQN_policy():\n",
    "    config.update({\n",
    "        \"n_step\": 1,\n",
    "        \"target_freq\": 1200,\n",
    "    })\n",
    "\n",
    "    action_count = len(Actions)\n",
    "    net = Q_network(action_count).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=config['lr'])\n",
    "    policy = CustomDQNPolicy(net, optim, discount_factor=config['gamma'], estimation_step=config['n_step'], target_update_freq=config['target_freq'], polyak=None)\n",
    "    # register_hooks(policy.model)\n",
    "    return policy\n",
    "\n",
    "def get_PPO_policy():\n",
    "    config.update({\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"vf_coef\": 0.25,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"rew_norm\": True,\n",
    "        \"eps_clip\": 0.1,\n",
    "        \"value_clip\": 1,\n",
    "        \"dual_clip\": None,\n",
    "        \"norm_adv\": True,\n",
    "        \"recompute_adv\": 0,\n",
    "    })\n",
    "\n",
    "    action_count = len(Actions)\n",
    "    base_net = ViTTrailEncoder()\n",
    "    actor = Actor(base_net, action_count, device=device, softmax_output=False)\n",
    "    critic = Critic(base_net, device=device)\n",
    "    optim = torch.optim.Adam(\n",
    "        ActorCritic(actor, critic).parameters(), lr=config['lr'], eps=1e-5\n",
    "    )\n",
    "    def dist(p):\n",
    "        return torch.distributions.Categorical(logits=p)\n",
    "    policy = PPOPolicy(\n",
    "        actor,\n",
    "        critic,\n",
    "        optim,\n",
    "        dist,\n",
    "        discount_factor=config['gamma'],\n",
    "        gae_lambda=config['gae_lambda'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        vf_coef=config['vf_coef'],\n",
    "        ent_coef=config['ent_coef'],\n",
    "        reward_normalization=config['rew_norm'],\n",
    "        action_scaling=False,\n",
    "        eps_clip=config['eps_clip'],\n",
    "        value_clip=config['value_clip'],\n",
    "        dual_clip=config['dual_clip'],\n",
    "        advantage_normalization=config['norm_adv'],\n",
    "        recompute_advantage=config['recompute_adv'],\n",
    "    ).to(device)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_offpolicy_trainer(policy):\n",
    "    replay_buffer = ts.data.VectorReplayBuffer(config['buffer_size'], len(config['train_envs']))\n",
    "    train_collector = ts.data.Collector(policy, config['train_envs'], replay_buffer, exploration_noise=True)\n",
    "\n",
    "    config.update({\n",
    "        \"eps_start\": 0.9,\n",
    "        \"eps_end\": 0.05,\n",
    "        'update_per_step': 1 / config['step_per_collect'] * 4,\n",
    "        \"eps_train\": np.linspace(0.9, 0.05, 2000),\n",
    "        \"eps_test\": 0,\n",
    "    })\n",
    "    trainer = CustomOffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=None,\n",
    "        episode_per_test=config['test_num'],\n",
    "        max_epoch=config['epoch'],\n",
    "        step_per_epoch=config['step_per_epoch'],\n",
    "        step_per_collect=config['step_per_collect'],\n",
    "        batch_size=config['batch_size'],\n",
    "        update_per_step=config['update_per_step'],\n",
    "        train_fn=get_train_fn(policy, config['eps_train'], config['initial_phase']),\n",
    "        test_fn=lambda epoch, env_step: policy.set_eps(config['eps_test']),\n",
    "        logger= wandb_logger,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def get_onpolicy_trainer(policy):\n",
    "    replay_buffer = ts.data.VectorReplayBuffer(\n",
    "        config['buffer_size'],\n",
    "        len(config['train_envs']),\n",
    "    )\n",
    "    train_collector = ts.data.Collector(policy, config['train_envs'], replay_buffer, exploration_noise=True)\n",
    "\n",
    "    config.update({\n",
    "        'repeat_per_collect': 1,\n",
    "        'episode_per_test': 5,\n",
    "    })\n",
    "\n",
    "    trainer = OnpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=None,\n",
    "        max_epoch=config['epoch'],\n",
    "        step_per_epoch=config['step_per_epoch'],\n",
    "        repeat_per_collect=config['repeat_per_collect'],\n",
    "        batch_size=config['batch_size'],\n",
    "        step_per_collect=config['step_per_collect'],\n",
    "        episode_per_test=config['episode_per_test'],\n",
    "        train_fn=get_train_fn(policy, None, config['initial_phase']),\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:28:08.578141786Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:28:02.566059669Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "ExecuteTime",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": ########Policy Initialization########\n",
    "    config = {\n",
    "        \"lr\": 8e-5,\n",
    "        \"epoch\": 2000,\n",
    "        \"batch_size\": 10,\n",
    "        \"initial_phase\": 0,\n",
    "        \"test_num\": 10,\n",
    "        \"gamma\": 0.98,\n",
    "        \"buffer_size\": 10000,\n",
    "        \"step_per_epoch\": 2000,\n",
    "        \"step_per_collect\": 100,\n",
    "        \"env_step_limit\": 100,\n",
    "    }\n",
    "\n",
    "    dataset = COCODataset(train=True, indices=[12], no_seg=True)\n",
    "    # train_envs = ts.env.SubprocVectorEnv([lambda: TimeLimit(Environment(dataset, (32, 32)), config['env_step_limit']) for _ in range(2)])\n",
    "    train_envs = ts.env.DummyVectorEnv([lambda: TimeLimit(Environment(dataset, (32, 32)), config['env_step_limit']) for _ in range(1)])\n",
    "    config['train_envs'] = train_envs\n",
    "\n",
    "    # policy = get_DQN_policy()\n",
    "    policy = get_PPO_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy.load_state_dict(torch.load('saved/policy_soft-totem-50.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": ########Training########\n",
    "    use_wandb = True\n",
    "    wandb_logger = get_wandb(use_wandb)\n",
    "\n",
    "    # trainer = get_offpolicy_trainer(policy)\n",
    "    trainer = get_onpolicy_trainer(policy)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb_logger.wandb_run.config.update(config)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     env = TimeLimit(Environment(dataset, (32, 32)), 200)\n",
    "#     ut.show_masks_on_image(einops.rearrange(env.current_image, 'c h w -> h w c'), env.current_seg.cpu().numpy())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # dataset = COCODataset(root=\"../Data/COCO17\", train=True, len=1)\n",
    "    policy.eval()\n",
    "    # policy.set_eps(config['eps_test'])\n",
    "    dataset = COCODataset(train=True, indices=[12], no_seg=True)\n",
    "    env = TimeLimit(Environment(dataset, (32, 32)), 100)\n",
    "    collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "    collector.collect(n_episode=1, render=1/10)\n",
    "    # collector.collect(n_episode=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
