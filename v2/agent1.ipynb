{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "from enum import Enum\n",
    "import random\n",
    "from typing import SupportsFloat, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.core import RenderFrame, ActType, ObsType\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from GradCam import GradCam\n",
    "from torchvision.models import resnet101, resnet50\n",
    "from utils import image_net_postprocessing\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "import torch.optim as optim\n",
    "import tianshou as ts\n",
    "from tianshou.utils import WandbLogger\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class COCODataset(D.Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        root = Path(root)\n",
    "\n",
    "        with open(root/'annotations/captions_train2017.json', 'r') as f:\n",
    "            images_info = json.load(f)\n",
    "        self.file_name_to_id = dict()\n",
    "        for image_info in images_info['images']:\n",
    "            self.file_name_to_id[image_info['file_name']] = image_info['id']\n",
    "\n",
    "        with open(root/'cap_dict.json', 'r') as f:\n",
    "            self.captions_dict = json.load(f)\n",
    "\n",
    "        if train:\n",
    "            self.image_files = glob.glob(os.path.join(root/'train2017', \"*.jpg\"))\n",
    "        else:\n",
    "            self.image_files = glob.glob(os.path.join(root/'val2017', \"*.jpg\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.image_files[index]\n",
    "        image = PIL.Image.open(image_file)\n",
    "        image = image.convert('RGB')\n",
    "        file_name = image_file.split('/')[-1]\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "        return image_tensor, self.file_name_to_id[file_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResnetGradCam(torch.nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(ResnetGradCam, self).__init__()\n",
    "        feature_extractor = resnet50(pretrained=True).to(device).eval()\n",
    "        self.vis = GradCam(feature_extractor, device)\n",
    "        self.patch_size = patch_size\n",
    "        self.conv_mean = nn.Conv2d(1, 1, kernel_size=self.patch_size, stride=self.patch_size, padding=0,  bias=False)\n",
    "        self.conv_mean.weight = torch.nn.Parameter(torch.ones_like(self.conv_mean.weight)  / (patch_size[0] * patch_size[1]))\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        '''\n",
    "        return the row/col number of a single patch chosen randomly based on the gradcam score\n",
    "        '''\n",
    "        self.vis(x, None, postprocessing=image_net_postprocessing)\n",
    "        self.vis.cam -= torch.min(self.vis.cam)\n",
    "        self.vis.cam /= torch.max(self.vis.cam)\n",
    "        resized_cam = transforms.Resize(size=(h, w))(self.vis.cam.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        number_of_rows = (h - self.patch_size[0]) // self.patch_size[0] + 1\n",
    "        number_of_cols = (w - self.patch_size[1]) // self.patch_size[1] + 1\n",
    "\n",
    "        scores = self.conv_mean(resized_cam)\n",
    "\n",
    "        choice = torch.multinomial(scores.flatten(), 1).item()\n",
    "\n",
    "        row_choice, col_choice = choice // number_of_cols, choice % number_of_cols\n",
    "\n",
    "        return torch.tensor([[row_choice, col_choice]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "    # STAY = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Environment(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, dataloader, patch_size=(64, 64), input_size=224):\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = iter(dataloader)\n",
    "        self.grad = ResnetGradCam(patch_size).to(device).eval()\n",
    "        self.transform = transforms.Resize(input_size)\n",
    "        self.patch_size = patch_size\n",
    "        self.reset()\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(3, self.patch_size[0], self.patch_size[1]), dtype=np.uint8)\n",
    "        self.action_space = spaces.Discrete(len(Actions))\n",
    "\n",
    "        self.row, self.col, self.max_row, self.max_col = None, None, None, None\n",
    "        self.height, self.width = None, None\n",
    "        self.current_image, self.image_id, self.captions = None, None, None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        try:\n",
    "            # Samples the batch\n",
    "            self.current_image, self.image_id = next(self.iterator)\n",
    "        except StopIteration:\n",
    "            # restart the iterator if the previous iterator is exhausted.\n",
    "            self.iterator = iter(self.dataloader)\n",
    "            self.current_image, self.image_id = next(self.iterator)\n",
    "\n",
    "        self.current_image = self.current_image\n",
    "        self.image_id = str(self.image_id.item())\n",
    "        _, _, self.height, self.width = self.current_image.shape\n",
    "        self.captions = self.dataloader.dataset.captions_dict[self.image_id]\n",
    "        self.row, self.col = [int(x) for x in self.grad(self.transform(self.current_image.to(device)), self.height, self.width)[0]] # just a single row/col\n",
    "        self.max_row, self.max_col = (self.height - self.patch_size[0]) // self.patch_size[0], (self.width - self.patch_size[1]) // self.patch_size[1]\n",
    "        # self.calculate_patch_histograms()\n",
    "        return self._get_patch(), {}\n",
    "\n",
    "    def _get_patch(self):\n",
    "        start_row, end_row = self.row * self.patch_size[0], (self.row + 1) * self.patch_size[0]\n",
    "        start_col, end_col = self.col * self.patch_size[1], (self.col + 1) * self.patch_size[1]\n",
    "        return self.current_image[0, :, start_row: end_row, start_col: end_col]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        if Actions(action) == Actions.UP:\n",
    "            self.row = self.row - 1 if self.row > 0 else self.row\n",
    "        elif Actions(action) == Actions.RIGHT:\n",
    "            self.col = self.col + 1 if self.col < self.max_col else self.col\n",
    "        elif Actions(action) == Actions.DOWN:\n",
    "            self.row = self.row + 1 if self.row < self.max_row else self.row\n",
    "        elif Actions(action) == Actions.LEFT:\n",
    "            self.col = self.col - 1 if self.col > 0 else self.col\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        patch = self._get_patch()\n",
    "        return patch, 0, False, False, {}\n",
    "\n",
    "    # def calculate_patch_histograms(self):\n",
    "    #     max_row, max_col = self.height // self.patch_size[0], self.width // self.patch_size[1]\n",
    "    #\n",
    "    #     self.histograms = []\n",
    "    #\n",
    "    #     for i in range(max_row):\n",
    "    #         self.histograms.append([])\n",
    "    #         for j in range(max_col):\n",
    "    #             start_row, end_row = i * self.patch_size[0], (i + 1) * self.patch_size[0]\n",
    "    #             start_col, end_col = j * self.patch_size[1], (j + 1) * self.patch_size[1]\n",
    "    #\n",
    "    #             selected_patch = self.current_image[:, :, start_row: end_row, start_col: end_col].detach().clone()\n",
    "    #\n",
    "    #             hue = self.rgb2h(selected_patch).cpu()\n",
    "    #             self.histograms[-1].append(np.histogram(hue, bins=10, range=(0., 1.), density=True)[0])\n",
    "    #\n",
    "    # def visualize(self, alpha=0.4):\n",
    "    #     start_row, end_row = self.row * self.patch_size[0], (self.row + 1) * self.patch_size[0]\n",
    "    #     start_col, end_col = self.col * self.patch_size[1], (self.col + 1) * self.patch_size[1]\n",
    "    #\n",
    "    #     mask = torch.zeros_like(self.current_image)\n",
    "    #     mask[:, :, start_row: end_row, start_col: end_col] = 1\n",
    "    #\n",
    "    #     return alpha * mask + (1 - alpha) * self.current_image\n",
    "    #\n",
    "    # def rgb2h(self, rgb: torch.Tensor) -> torch.Tensor:\n",
    "    #     cmax, cmax_idx = torch.max(rgb, dim=1, keepdim=True)\n",
    "    #     cmin = torch.min(rgb, dim=1, keepdim=True)[0]\n",
    "    #     delta = cmax - cmin\n",
    "    #     hsv_h = torch.empty_like(rgb[:, 0:1, :, :])\n",
    "    #     cmax_idx[delta == 0] = 3\n",
    "    #     hsv_h[cmax_idx == 0] = (((rgb[:, 1:2] - rgb[:, 2:3]) / delta) % 6)[cmax_idx == 0]\n",
    "    #     hsv_h[cmax_idx == 1] = (((rgb[:, 2:3] - rgb[:, 0:1]) / delta) + 2)[cmax_idx == 1]\n",
    "    #     hsv_h[cmax_idx == 2] = (((rgb[:, 0:1] - rgb[:, 1:2]) / delta) + 4)[cmax_idx == 2]\n",
    "    #     hsv_h[cmax_idx == 3] = 0.\n",
    "    #     hsv_h /= 6.\n",
    "    #     hsv_s = torch.where(cmax == 0, torch.tensor(0.).type_as(rgb), delta / cmax)\n",
    "    #     hsv_v = cmax\n",
    "    #     return hsv_h\n",
    "    #\n",
    "    # def measure_similarity(self):\n",
    "    #     current_patch_histogram = self.histograms[self.row][self.col]\n",
    "    #     max_row, max_col = self.height // self.patch_size[0], self.width // self.patch_size[1]\n",
    "    #     similarity_matrix = []\n",
    "    #\n",
    "    #     for i in range(max_row):\n",
    "    #         similarity_matrix.append([])\n",
    "    #         for j in range(max_col):\n",
    "    #             p_value = ks_2samp(current_patch_histogram,  self.histograms[i][j]).pvalue\n",
    "    #             similarity_matrix[-1].append(p_value)\n",
    "    #\n",
    "    #     return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = COCODataset(root=\"../Data/COCO17\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: Environment(dataloader) for _ in range(1)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: Environment(dataloader) for _ in range(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.feature_extractor = resnet50(pretrained=True)\n",
    "        input_dim = self.feature_extractor.fc.out_features\n",
    "        action_shape = train_envs.get_env_attr('action_space',0)[0].n\n",
    "        self.dueling_head = ts.utils.net.common.Net(input_dim, action_shape, dueling_param=(\n",
    "            {\n",
    "                \"input_dim\": 1024,\n",
    "                \"output_dim\": action_shape,\n",
    "                \"hidden_sizes\": [512, 512],\n",
    "            },\n",
    "            {\n",
    "                \"input_dim\": 1024,\n",
    "                \"output_dim\": 1,\n",
    "                \"hidden_sizes\": [512, 512],\n",
    "            }\n",
    "        ), hidden_sizes=[1024],)\n",
    "\n",
    "    def forward(self, obs, **kwargs):\n",
    "        obs = torch.tensor(obs)\n",
    "        duel_out = self.dueling_head(self.feature_extractor(obs))\n",
    "        return duel_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "net = Q_network()\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "policy = ts.policy.DQNPolicy(net, optim, discount_factor=0.99, estimation_step=3, target_update_freq=32000)\n",
    "replay_buffer = ts.data.VectorReplayBuffer(100000, 1)\n",
    "train_collector = ts.data.Collector(policy, train_envs, replay_buffer)\n",
    "test_collector = ts.data.Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mattentionrl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "fatal: unable to create temp-file: Permission denied\n",
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/external_10TB/10TB/Behnamnia/AttentionRL/v2/wandb/run-20230511_085540-58142682</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://213.233.184.150:8080/attentionrl/AttentionRL/runs/58142682' target=\"_blank\">testname</a></strong> to <a href='http://213.233.184.150:8080/attentionrl/AttentionRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://213.233.184.150:8080/attentionrl/AttentionRL' target=\"_blank\">http://213.233.184.150:8080/attentionrl/AttentionRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://213.233.184.150:8080/attentionrl/AttentionRL/runs/58142682' target=\"_blank\">http://213.233.184.150:8080/attentionrl/AttentionRL/runs/58142682</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/behnamnia/.conda/envs/rl39/lib/python3.9/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "lr, epoch, batch_size = 1e-3, 10, 8\n",
    "train_num, test_num = 10, 10\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10, 10\n",
    "logger = WandbLogger(\n",
    "    train_interval=1,\n",
    "    test_interval=1,\n",
    "    update_interval=1,\n",
    "    project=\"AttentionRL\",\n",
    "    name=\"testname\",\n",
    ")\n",
    "writer = SummaryWriter(\"./logs\")\n",
    "logger.load(writer)\n",
    "\n",
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, epoch, step_per_epoch, step_per_collect,\n",
    "    test_num, batch_size, update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= 0,\n",
    "    logger=logger\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
