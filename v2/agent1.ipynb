{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.922521172Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.113529464Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.931609958Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.124617050Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script pack_existing_segs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.300770945Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:50.921840758Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.335443232Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:30.932201025Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "import einops\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tianshou.policy import PPOPolicy\n",
    "import uuid\n",
    "from tianshou.utils import WandbLogger, LazyLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from customs import CustomDQNPolicy, CustomOffpolicyTrainer\n",
    "from dataset import *\n",
    "from env import *\n",
    "from networks.q_network import *\n",
    "from networks.SimpleAC import Actor as SimpleActor, Critic as SimpleCritic\n",
    "from tianshou.utils.net.common import ActorCritic\n",
    "from tianshou.utils.net.discrete import Actor, Critic, IntrinsicCuriosityModule\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "import torch\n",
    "import tianshou as ts\n",
    "from networks.AC import Actor as MulActor, Critic as MulCritic, BaseNetwork as MulBaseNetwork\n",
    "import time as ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.301058023Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.300294091Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.335729135Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.331166072Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# if username is server use cuda\n",
    "if os.environ['USER'] == 'server':\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# if __name__ == '__main__':\n",
    "#     torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wandb(use_wandb):\n",
    "    if use_wandb:\n",
    "        wandb_logger = WandbLogger(\n",
    "            train_interval=1,\n",
    "            test_interval=1,\n",
    "            update_interval=1,\n",
    "            project=\"AttentionRL\",\n",
    "        )\n",
    "        wandb_logger.load(SummaryWriter(\"./logs\"))\n",
    "    else:\n",
    "        wandb_logger = LazyLogger()\n",
    "\n",
    "    return wandb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_fn(policy, eps_train, inital_phase):\n",
    "    def train_fn(epoch, env_step):\n",
    "        if eps_train:\n",
    "            policy.set_eps(eps_train[epoch - 1])\n",
    "\n",
    "        if epoch >= inital_phase:\n",
    "            for param in policy.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        if epoch >= 0 and epoch % 5 == 0 and use_wandb:\n",
    "            torch.save(policy.state_dict(),\n",
    "                       f'saved/policy_{wandb_logger.wandb_run.name if use_wandb else uuid.uuid4().hex}.pt')\n",
    "\n",
    "        wandb_logger.write('train', trainer.env_step, {\"epoch\": epoch})\n",
    "\n",
    "    return train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.378901552Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-15T16:27:52.349735874Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.378122737Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-07-12T15:29:32.375907458Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "def register_hooks(model):\n",
    "    hook0 = model.dueling_head.Q.model[-1].register_forward_hook(lambda self, input, output: print(f\"Q: {output}\"))\n",
    "    hook1 = model.dueling_head.V.model[-1].register_forward_hook(lambda self, input, output: print(f\"V: {output}\"))\n",
    "    hook2 = model.dueling_head.model.model[0].register_forward_hook(lambda self, input, output: print(f\"net: {output}\"))\n",
    "\n",
    "\n",
    "def get_DQN_policy():\n",
    "    config.update({\n",
    "        \"n_step\": 1,\n",
    "        \"target_freq\": 1200,\n",
    "    })\n",
    "\n",
    "    action_count = len(Actions)\n",
    "    net = Q_network(action_count).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=config['lr'])\n",
    "    policy = CustomDQNPolicy(net, optim, discount_factor=config['gamma'], estimation_step=config['n_step'],\n",
    "                             target_update_freq=config['target_freq'], polyak=None)\n",
    "    # register_hooks(policy.model)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def get_PPO_policy():\n",
    "    config.update({\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"vf_coef\": 0.25,\n",
    "        \"ent_coef\": .01,\n",
    "        \"rew_norm\": True,\n",
    "        \"eps_clip\": 0.1,\n",
    "        \"value_clip\": True,\n",
    "        \"dual_clip\": None,\n",
    "        \"norm_adv\": True,\n",
    "        \"recompute_adv\": 0,\n",
    "    })\n",
    "    \n",
    "    # old base, built-in actor and critic\n",
    "    # base_net = ViTTrailEncoder()\n",
    "    # actor = Actor(base_net, len(Actions), device=device, softmax_output=False, hidden_sizes=[])\n",
    "    # critic = Critic(base_net, device=device, hidden_sizes=[])    \n",
    "\n",
    "    # old base, improved built-in actor and critic\n",
    "    # base_net = ViTTrailEncoder()\n",
    "    # actor = SimpleActor(base_net, len(Actions), device=device, softmax_output=False, hidden_sizes=[256])\n",
    "    # critic = SimpleCritic(base_net, device=device, hidden_sizes=[512])\n",
    "\n",
    "    # new base, built-in actor and critic\n",
    "    # base_net = MulBaseNetwork(config['env_patch_size']).to(device)\n",
    "    # actor = Actor(base_net, len(Actions), device=device, softmax_output=False)\n",
    "    # critic = Critic(base_net, device=device)\n",
    "\n",
    "    # new base, improved built-in actor and critic\n",
    "    base_net = MulBaseNetwork(config['env_patch_size']).to(device)\n",
    "    actor = SimpleActor(base_net, len(Actions), device=device, softmax_output=False, hidden_sizes=[256])\n",
    "    critic = SimpleCritic(base_net, device=device, hidden_sizes=[512])\n",
    "\n",
    "    # new base, custom actor and critic\n",
    "    # base_net = MulBaseNetwork(config['env_patch_size']).to(device)\n",
    "    # actor = MulActor(base_net).to(device)\n",
    "    # critic = MulCritic(base_net).to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(\n",
    "        ActorCritic(actor, critic).parameters(), lr=config['lr'], eps=1e-5\n",
    "    )\n",
    "\n",
    "    def dist(p):\n",
    "        return torch.distributions.Categorical(logits=p)\n",
    "\n",
    "    # policy = PPOPolicy(\n",
    "    policy = TempPPO(\n",
    "        actor=actor,\n",
    "        critic=critic,\n",
    "        optim=optim,\n",
    "        dist_fn=dist,\n",
    "        discount_factor=config['gamma'],\n",
    "        gae_lambda=config['gae_lambda'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        vf_coef=config['vf_coef'],\n",
    "        ent_coef=config['ent_coef'],\n",
    "        reward_normalization=config['rew_norm'],\n",
    "        action_scaling=False,\n",
    "        eps_clip=config['eps_clip'],\n",
    "        value_clip=config['value_clip'],\n",
    "        dual_clip=config['dual_clip'],\n",
    "        advantage_normalization=config['norm_adv'],\n",
    "        recompute_advantage=config['recompute_adv'],\n",
    "    ).to(device)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "from tianshou.data import Batch\n",
    "\n",
    "\n",
    "class TempPPO(PPOPolicy):\n",
    "    def learn(  # type: ignore\n",
    "            self, batch: Batch, batch_size: int, repeat: int, **kwargs: Any\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        losses, clip_losses, vf_losses, ent_losses = [], [], [], []\n",
    "        for step in range(repeat):\n",
    "            if self._recompute_adv and step > 0:\n",
    "                batch = self._compute_returns(batch, self._buffer, self._indices)\n",
    "            for minibatch in batch.split(batch_size, merge_last=True):\n",
    "                # calculate loss for actor\n",
    "                dist = self(minibatch).dist\n",
    "                if self._norm_adv:\n",
    "                    mean, std = minibatch.adv.mean(), minibatch.adv.std()\n",
    "                    minibatch.adv = (minibatch.adv -\n",
    "                                     mean) / (std + self._eps)  # per-batch norm\n",
    "                ratio = (dist.log_prob(minibatch.act) -\n",
    "                         minibatch.logp_old).exp().float()\n",
    "                ratio = ratio.reshape(ratio.size(0), -1).transpose(0, 1)\n",
    "                surr1 = ratio * minibatch.adv\n",
    "                surr2 = ratio.clamp(\n",
    "                    1.0 - self._eps_clip, 1.0 + self._eps_clip\n",
    "                ) * minibatch.adv\n",
    "                if self._dual_clip:\n",
    "                    clip1 = torch.min(surr1, surr2)\n",
    "                    clip2 = torch.max(clip1, self._dual_clip * minibatch.adv)\n",
    "                    clip_loss = -torch.where(minibatch.adv < 0, clip2, clip1).mean()\n",
    "                else:\n",
    "                    clip_loss = -torch.min(surr1, surr2).mean()\n",
    "                # calculate loss for critic\n",
    "                value = self.critic(minibatch.obs).flatten()\n",
    "                if self._value_clip:\n",
    "                    v_clip = minibatch.v_s + \\\n",
    "                             (value - minibatch.v_s).clamp(-self._eps_clip, self._eps_clip)\n",
    "                    vf1 = (minibatch.returns - value).pow(2)\n",
    "                    vf2 = (minibatch.returns - v_clip).pow(2)\n",
    "                    vf_loss = torch.max(vf1, vf2).mean()\n",
    "                else:\n",
    "                    vf_loss = (minibatch.returns - value).pow(2).mean()\n",
    "                # calculate regularization and overall loss\n",
    "                ent_loss = dist.entropy().mean()\n",
    "                loss = clip_loss + self._weight_vf * vf_loss \\\n",
    "                       - self._weight_ent * ent_loss\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                if self._grad_norm:  # clip large gradient\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self._actor_critic.parameters(), max_norm=self._grad_norm\n",
    "                    )\n",
    "                self.optim.step()\n",
    "                clip_losses.append(clip_loss.item())\n",
    "                vf_losses.append(vf_loss.item())\n",
    "                ent_losses.append(ent_loss.item())\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return {\n",
    "            \"loss\": losses,\n",
    "            \"loss/clip\": clip_losses,\n",
    "            \"loss/vf\": vf_losses,\n",
    "            \"loss/ent\": ent_losses,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_offpolicy_trainer(policy):\n",
    "    replay_buffer = ts.data.VectorReplayBuffer(config['buffer_size'], len(config['train_envs']))\n",
    "    train_collector = ts.data.Collector(policy, config['train_envs'], replay_buffer, exploration_noise=True)\n",
    "\n",
    "    config.update({\n",
    "        \"eps_start\": 0.9,\n",
    "        \"eps_end\": 0.05,\n",
    "        'update_per_step': 1 / config['step_per_collect'] * 4,\n",
    "        \"eps_train\": np.linspace(0.9, 0.05, 2000),\n",
    "        \"eps_test\": 0,\n",
    "    })\n",
    "    trainer = CustomOffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=None,\n",
    "        episode_per_test=1, #dummy\n",
    "        max_epoch=config['epoch'],\n",
    "        step_per_epoch=config['step_per_epoch'],\n",
    "        step_per_collect=config['step_per_collect'],\n",
    "        batch_size=config['batch_size'],\n",
    "        update_per_step=config['update_per_step'],\n",
    "        train_fn=get_train_fn(policy, config['eps_train'], config['initial_phase']),\n",
    "        test_fn=lambda epoch, env_step: policy.set_eps(config['eps_test']),\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def get_onpolicy_trainer(policy):\n",
    "    replay_buffer = ts.data.VectorReplayBuffer(\n",
    "        config['buffer_size'],\n",
    "        len(config['train_envs']),\n",
    "    )\n",
    "    train_collector = ts.data.Collector(policy, config['train_envs'], replay_buffer, exploration_noise=True)\n",
    "    test_collector = ts.data.Collector(policy, config['test_envs'])\n",
    "\n",
    "    config.update({\n",
    "        'repeat_per_collect': 1,\n",
    "        'episode_per_test': 5,\n",
    "    })\n",
    "\n",
    "    trainer = OnpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=config['epoch'],\n",
    "        step_per_epoch=config['step_per_epoch'],\n",
    "        repeat_per_collect=config['repeat_per_collect'],\n",
    "        batch_size=config['batch_size'],\n",
    "        step_per_collect=config['step_per_collect'],\n",
    "        episode_per_test=config['episode_per_test'],\n",
    "        train_fn=get_train_fn(policy, None, config['initial_phase']),\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  ########Policy Initialization########\n",
    "    config = {\n",
    "        \"lr\": 8e-5,\n",
    "        \"epoch\": 2000,\n",
    "        \"batch_size\": 64,\n",
    "        \"initial_phase\": 2000,\n",
    "        \"gamma\": 0.98,\n",
    "        \"buffer_size\": 10000,\n",
    "        \"env_step_limit\": 100,\n",
    "        \"env_step_limit_test\": 100,\n",
    "        \"env_step_mem\": 100,\n",
    "        \"env_patch_size\": (32, 32),\n",
    "        'num_train_envs': 1,\n",
    "        'step_per_collect': 32 * 6,\n",
    "        \"step_per_epoch\": 20 * 32 * 6,\n",
    "    }\n",
    "\n",
    "    dataset = COCODataset(train=True, indices=[12], no_seg=True)\n",
    "    \n",
    "    if config['num_train_envs'] > 1:\n",
    "        train_envs = ts.env.SubprocVectorEnv([lambda: TimeLimit(Environment(dataset, config['env_patch_size'], max_len=config['env_step_mem']), config['env_step_limit']) for _ in range(config['num_train_envs'])])\n",
    "    else:\n",
    "        train_envs = ts.env.DummyVectorEnv(\n",
    "            [lambda: TimeLimit(Environment(dataset, config['env_patch_size'], max_len=config['env_step_mem']), config['env_step_limit']) for _ in range(1)])\n",
    "    config['train_envs'] = train_envs\n",
    "    config['test_envs'] = ts.env.DummyVectorEnv(\n",
    "        [lambda: TimeLimit(Environment(dataset, config['env_patch_size'], max_len=config['env_step_mem']), config['env_step_limit_test']) for _ in range(1)])\n",
    "\n",
    "    # policy = get_DQN_policy()\n",
    "    policy = get_PPO_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# policy.load_state_dict(torch.load(\"saved/policy_super-pond-83.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  ########Training########\n",
    "    use_wandb = True\n",
    "    wandb_logger = get_wandb(use_wandb)\n",
    "\n",
    "    # trainer = get_offpolicy_trainer(policy)\n",
    "    trainer = get_onpolicy_trainer(policy)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb_logger.wandb_run.config.update(config)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # if __name__ == \"__main__\":\n",
    "#     env = TimeLimit(Environment(dataset, (32, 32)), 200)\n",
    "#     ut.show_masks_on_image(einops.rearrange(env.current_image, 'c h w -> h w c'), env.current_seg.cpu().numpy())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # dataset = COCODataset(root=\"../Data/COCO17\", train=True, len=1)\n",
    "    policy.eval()\n",
    "    # policy.set_eps(config['eps_test'])\n",
    "    dataset = COCODataset(train=True, indices=[16], no_seg=True)\n",
    "    env = TimeLimit(Environment(dataset, config['env_patch_size'], max_len=config['env_step_mem']), config['env_step_limit_test'])\n",
    "    # env = TimeLimit(Environment(dataset, config['env_patch_size']), 100)\n",
    "    collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "    collector.collect(n_episode=1, render=1 / 5)\n",
    "    # collector.collect(n_episode=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
